{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b2cb5-a7c7-423b-84e6-8ff49eac2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  git+https://github.com/CompVis/taming-transformers.git\n",
    "!git clone https://github.com/CompVis/taming-transformers.git && cd taming-transformers && python -m pip install -e .\n",
    "%pip install \"omegaconf==2.0.0\" \"pytorch-lightning==1.0.8\" einops transformers\n",
    "%pip install -q git+https://github.com/cthiounn/dalle-tiny.git\n",
    "from tqdm import tqdm\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "BUCKET = \"cthiounn2\"\n",
    "fs.ls(BUCKET)\n",
    "\n",
    "files=['checkpoint_reverse_pokemon_launch2022_05_28_10248.pth','model_vqgan_minidalle.ckpt','config_vqgan_minidalle.yaml']\n",
    "for file in tqdm(files):\n",
    "    with fs.open(f'{BUCKET}/{file}', mode=\"rb\") as file_in, open(file,\"wb\") as file_out:\n",
    "            file_out.write(file_in.read())\n",
    "\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb948ef0-ad8c-4f8c-81e6-86de42c8c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "from omegaconf import OmegaConf\n",
    "import taming\n",
    "from taming.models.vqgan import VQModel\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "config_path = \"config_vqgan_minidalle.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "vqmodel=VQModel(**config.model.params).to(DEVICE)\n",
    "\n",
    "ckpt_path = \"model_vqgan_minidalle.ckpt\"\n",
    "\n",
    "sd = torch.load(ckpt_path, map_location=DEVICE)[\"state_dict\"]\n",
    "vqmodel.load_state_dict(sd, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5ee67-9a26-4764-bf42-bc508b9276a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalle_tiny.model import TinyDalleModel\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "try:\n",
    "    model=TinyDalleModel.from_pretrained('.')\n",
    "except:\n",
    "    model=TinyDalleModel.from_pretrained('facebook/bart-large-cnn')\n",
    "model.reinit_model_for_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06ca0f-7665-4a4f-896d-48e11778e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions=[]\n",
    "captions.append(\"Pikachu et Dragofeu\")\n",
    "captions.append(\"Pikachu\")\n",
    "captions.append(\"Bulb i zarre\")\n",
    "captions.append(\"Bulb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d68929-048c-4334-b478-24405f4d8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import BartTokenizer\n",
    "from collections import defaultdict\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "image_dict=defaultdict(list)\n",
    "# file_in=\"checkpoint_test_2022_05_02_191000.pth\"\n",
    "file_in=\"checkpoint_reverse_pokemon_launch2022_05_28_10248.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(file_in,map_location=device))\n",
    "model.config.eos_token_id=16384\n",
    "model.config.max_length=257\n",
    "model.eval()\n",
    "model=model.to(device)\n",
    "\n",
    "num_ret=10\n",
    "\n",
    "images=[]\n",
    "for j,caption in enumerate(captions):\n",
    "  inputs=tokenizer(caption, return_tensors=\"pt\",max_length=256,padding=\"max_length\")\n",
    "  inputs=inputs.to(device)\n",
    "  pred=model.generate(**inputs,\n",
    "                      # do_sample=False,\n",
    "                      # num_beams=10,\n",
    "                      do_sample=True,\n",
    "                      top_p=0.90,\n",
    "                      temperature=1.7,\n",
    "                      top_k=300,\n",
    "                      # top_k=25,\n",
    "\n",
    "                      num_return_sequences=num_ret\n",
    "                      )\n",
    "  pred=pred.detach()\n",
    "  pred=pred.squeeze()\n",
    "  \n",
    "  output_indices=torch.Tensor(num_ret,256)\n",
    "  output_indices[:]=0\n",
    "  output_indices[:,:256]=pred[:,1:]\n",
    "  output_indices=output_indices.to(torch.long)\n",
    "  output_indices.to(\"cpu\")\n",
    "  vqmodel=vqmodel.to(\"cpu\")\n",
    "\n",
    "\n",
    "  z_q = vqmodel.quantize.embedding(output_indices).reshape(num_ret, 16, 16, 256).permute(0,3,1,2)\n",
    "  u=vqmodel.decode(z_q).add(1).div(2).cpu()\n",
    "  image_dict[j].append(u)\n",
    "  video_file=caption.replace('.','').replace(' ','_').lower()\n",
    "  for i in range(num_ret):\n",
    "    img=u[i,:,:,:].cpu().squeeze().permute(1, 2, 0)\n",
    "    img.squeeze().clip(0.0, 1.0)\n",
    "    \n",
    "    ii=np.asarray(img * 255, dtype=np.uint8)\n",
    "\n",
    "\n",
    "    aa=Image.fromarray(ii)\n",
    "    images.append(ii)\n",
    "    aa.save(video_file+str(i)+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8cbfd-a116-4d4a-96d7-8a1e775da282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(array, ncols=5):\n",
    "    index, height, width, channels = array.shape\n",
    "    nrows = index//ncols\n",
    "    img_grid = (array.reshape(nrows, ncols, height, width, channels)\n",
    "              .swapaxes(1,2)\n",
    "              .reshape(height*nrows, width*ncols,channels))\n",
    "    \n",
    "    return img_grid\n",
    "\n",
    "result = image_grid(np.array(images))\n",
    "fig = plt.figure(figsize=(40., 40.))\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41096b54-04d0-459c-ae11-bc9d174d0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf *.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
