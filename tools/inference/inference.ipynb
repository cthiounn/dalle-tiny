{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02071ca-bd7a-4471-8287-57e04339bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/CompVis/taming-transformers.git\n",
    "!git clone https://github.com/CompVis/taming-transformers.git && cd taming-transformers && python -m pip install -e .\n",
    "%pip install -q \"omegaconf>=2.0.0\" \"pytorch-lightning>=1.0.8\" einops transformers imageio-ffmpeg\n",
    "%pip install -q git+https://github.com/cthiounn/dalle-tiny.git\n",
    "\n",
    "# !curl -L \"https://huggingface.co/boris/vqgan_f16_16384/raw/main/config.yaml\" > \"config_vqgan_minidalle.yaml\"\n",
    "# !curl -L \"https://huggingface.co/boris/vqgan_f16_16384/resolve/main/model.ckpt\" > \"model_vqgan_minidalle.ckpt\"\n",
    "\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3acbe-ea3c-4361-95e6-d19bd1211d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "BUCKET = \"cthiounn2\"\n",
    "fs.ls(BUCKET)\n",
    "\n",
    "files=['model_vqgan_minidalle.ckpt','config_vqgan_minidalle.yaml','config.json','pytorch_model.bin']\n",
    "for file in tqdm(files):\n",
    "    with fs.open(f'{BUCKET}/{file}', mode=\"rb\") as file_in, open(file,\"wb\") as file_out:\n",
    "            file_out.write(file_in.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b056556-2dab-4820-ab9b-b12987a56e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "from omegaconf import OmegaConf\n",
    "import taming\n",
    "from taming.models.vqgan import VQModel\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "config_path = \"config_vqgan_minidalle.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "vqmodel=VQModel(**config.model.params).to(DEVICE)\n",
    "\n",
    "ckpt_path = \"model_vqgan_minidalle.ckpt\"\n",
    "\n",
    "sd = torch.load(ckpt_path, map_location=DEVICE)[\"state_dict\"]\n",
    "vqmodel.load_state_dict(sd, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb122c-641f-42a8-b5ff-6618928b2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalle_tiny.model import TinyDalleModel\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "try:\n",
    "    model=TinyDalleModel.from_pretrained('.')\n",
    "except:\n",
    "    model=TinyDalleModel.from_pretrained('facebook/bart-large-cnn')\n",
    "model.reinit_model_for_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32445bf7-bb5a-42fc-9c8c-69e889e05fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions=[]\n",
    "captions.append(\"A black clock in snowy area with building in background.\")\n",
    "captions.append(\"A city bus being followed by a red car.\")\n",
    "captions.append(\"A full view of a beautiful store in a town.\")\n",
    "captions.append(\"Family and friends are together at the beach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4423dd7-4d70-4d5b-9625-f5f5d360cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import BartTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "image_dict=defaultdict(list)\n",
    "for i in range(0,32):\n",
    "    print(i)\n",
    "    FILE_KEY_S3 = f\"checkpoint_fixdecoderidtoken_{i*5}.pth\"\n",
    "    FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "    try:\n",
    "        with fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n",
    "\n",
    "            model.load_state_dict(torch.load(file_in,map_location=device))\n",
    "            model.config.eos_token_id=16384\n",
    "            model.config.max_length=257\n",
    "        model.eval()\n",
    "        model=model.to(device)\n",
    "        \n",
    "        for j,caption in enumerate(captions):\n",
    "            inputs=tokenizer(caption, return_tensors=\"pt\",max_length=257,padding=\"max_length\")\n",
    "            inputs=inputs.to(device)\n",
    "            pred=model.generate(**inputs, do_sample=True, top_k=100)\n",
    "            pred=pred.detach()\n",
    "            pred=pred.squeeze()\n",
    "            output_indices=torch.Tensor(256)\n",
    "            output_indices[:]=0\n",
    "            output_indices[:256]=pred[1:]\n",
    "            output_indices=output_indices.to(torch.long)\n",
    "            output_indices.to(\"cpu\")\n",
    "            vqmodel=vqmodel.to(\"cpu\")\n",
    "\n",
    "\n",
    "            z_q = vqmodel.quantize.embedding(output_indices).reshape(1, 16, 16, 256).permute(0,3,1,2)\n",
    "            u=vqmodel.decode(z_q).add(1).div(2).cpu().squeeze().permute(1, 2, 0)\n",
    "            image_dict[j].append(u)\n",
    "    except:\n",
    "        print(f\"issue with {i} : {FILE_KEY_S3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c416a-00ce-42f4-a302-03d433a1d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "for j,caption in enumerate(captions):\n",
    "    video_file=caption.replace('.','').replace(' ','_').lower()\n",
    "    writer = imageio.get_writer(video_file + '.mp4', fps=1)\n",
    "    for im in image_dict[j]:\n",
    "        writer.append_data(np.array(im))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c00936-720f-497f-b6d2-f87588a146b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "for j,caption in enumerate(captions):\n",
    "    video_file=caption.replace('.','').replace(' ','_').lower()\n",
    "    mp4 = open(video_file+\".mp4\",'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    print(caption)\n",
    "    display(HTML(\"\"\"\n",
    "    <video width=500  autoplay=\"autoplay\" controls muted>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
